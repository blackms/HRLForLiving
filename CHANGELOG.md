# Changelog

All notable changes to the Personal Finance Optimization HRL System will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Changed
- Updated `analyze_strategy.py` to use `personal_realistic` configuration and models instead of `personal_eur`
  - Now loads from `configs/personal_realistic.yaml`
  - Now loads models from `models/personal_realistic_high_agent.pt` and `models/personal_realistic_low_agent.pt`

### Added
- Explainable AI analysis script (`explain_failure.py`)
  - Detailed month-by-month breakdown of agent behavior
  - Shows WHY the agent fails and WHERE problems occur
  - Displays initial situation (income, expenses, buffer, available funds)
  - Month-by-month cash flow analysis:
    - Cash balance at start of month
    - Income and all expenses (fixed, variable, investment)
    - Net cash flow and final balance
    - Warnings when approaching failure conditions
  - Failure analysis when cash goes negative:
    - Structural problem identification (income vs expenses vs investment)
    - Buffer consumption rate calculation
    - Inflation impact over time
  - Sustainable strategy recommendations:
    - Maximum sustainable investment rate calculation
    - Options to increase available funds (reduce expenses, increase income)
    - Buffer management strategies
  - Outputs in Italian for personal finance context
  - Configurable via model paths and configuration files
  - Useful for understanding agent failures and debugging training issues
  - Provides explainable AI insights for non-technical stakeholders
- Strategy analysis script (`analyze_strategy.py`)
  - Loads trained models and runs deterministic simulation
  - Analyzes learned financial strategy and decision patterns
  - Displays initial situation summary (income, expenses, available funds)
  - Shows simulation results (allocation ratios, financial outcomes)
  - Provides practical recommendations:
    - Monthly allocation breakdown in currency
    - Recommended safety buffer amount
    - Risk profile assessment (Conservative/Moderate/Aggressive)
    - Long-term sustainability evaluation
  - Outputs in Italian for personal finance context
  - Configurable via model paths and configuration files
  - Useful for understanding learned policies and extracting actionable advice
- State normalization in FinancialStrategist.aggregate_state() for training stability
  - Normalizes all 5 aggregated state features to prevent extreme values
  - avg_cash normalized by 10000.0 → ~0.5-1.0 range
  - avg_investment_return normalized by 1000.0 → ~-0.5 to 0.5 range
  - spending_trend normalized by 100.0 → ~-0.1 to 0.1 range
  - current_wealth normalized by 10000.0 → ~0.5-1.0 range
  - months_elapsed normalized by 120.0 → [0, 1] range
  - NaN/Inf safety checks with fallback to default state
  - Literature-based approach: "State normalization is critical for hierarchical RL" (Nachum et al., 2018 - HIRO)
  - Prevents training instability in high-level agent
- Reward scaling in RewardEngine to prevent gradient explosion
  - Low-level rewards automatically scaled by 1000.0
  - Prevents numerical instability with large income values (~$3200)
  - Brings rewards into recommended range [-10, 10] for stable training
  - Added NaN/Inf safety checks with fallback penalties
  - Literature-based approach for neural network training stability
- Simplified debug_nan.py script for environment and reward testing
  - Focuses on basic environment functionality without agent complexity
  - Tests BudgetEnv and RewardEngine for NaN values
  - Runs 12-step test episode with balanced actions
  - Displays step-by-step cash flow and reward information
  - Provides early NaN detection with diagnostic output
  - Useful for configuration validation and troubleshooting
- Comprehensive README documentation (Task 17)
  - Detailed behavioral profile descriptions with use cases and characteristics
  - Complete configuration parameters reference with tables and ranges
  - Environment, training, and reward parameter documentation
  - Configuration tips and best practices for each parameter type
  - Reward formula documentation with mathematical notation
  - Advanced training examples (checkpointing, resuming, custom directories)
  - Comprehensive troubleshooting section with common issues and solutions
  - Frequently Asked Questions (FAQ) section
  - System extension guide (custom rewards, metrics, profiles, architectures)
  - Contributing guidelines with code style and testing requirements
  - Future enhancements roadmap
  - Citation information for research use
  - Acknowledgments and references to foundational papers
- Quick Start Guide (QUICK_START.md)
  - 5-minute getting started guide for new users
  - Step-by-step installation and first training
  - Evaluation and TensorBoard visualization instructions
  - Next steps and customization guidance
  - Common first-time issues and solutions
  - Key metrics interpretation table
  - Typical training progress timeline
- Checkpointing and resume functionality (`src/training/hrl_trainer.py`)
  - Added imports: `os`, `json` for file operations
  - Added `Tuple` type hint for return types
  - Added `EnvironmentConfig` and `RewardConfig` imports for checkpoint metadata
  - `save_checkpoint()` method for saving training state
  - `load_checkpoint()` method for resuming training
  - `train_with_checkpointing()` method for automatic checkpoint management
  - Saves high-level and low-level agent models
  - Saves complete configuration (environment, training, reward)
  - Saves training history and current episode number
  - Tracks and saves best model based on evaluation performance
  - Supports custom checkpoint directories and naming
  - Automatic evaluation at configurable intervals
  - Best model selection based on mean reward during evaluation
  - Handles NaN evaluation scores gracefully
- Checkpointing support in train.py
  - Command-line options: --checkpoint-dir, --resume, --eval-interval, --eval-episodes-during-training
  - Automatic checkpoint directory creation
  - Resume training from saved checkpoints
  - Comprehensive checkpoint metadata preservation
  - Integration with existing training workflow
- Checkpointing tests (`tests/test_checkpointing.py`)
  - 7 comprehensive test cases covering all checkpointing functionality
  - Test checkpoint saving and loading
  - Test best model tracking
  - Test resume training functionality
  - Test train_with_checkpointing method
  - Test checkpoint metadata completeness
  - Test training history preservation
- Checkpointing usage example (`examples/checkpointing_usage.py`)
  - Complete demonstration of checkpointing workflow
  - Training with automatic checkpointing
  - Resuming training from checkpoint
  - Loading and evaluating best model
  - Updated examples/README.md with checkpointing documentation
- TensorBoard dependency (`tensorboard>=2.14.0`) added to requirements.txt
- TensorBoard logging and monitoring system (`src/utils/logger.py`)
  - ExperimentLogger class for comprehensive experiment tracking
  - Automatic logging of training curves (rewards, losses)
  - Episode metrics tracking (wealth, stability, Sharpe ratio)
  - Action and goal distribution visualization with histograms
  - Hyperparameter logging for reproducibility
  - Real-time monitoring with TensorBoard web interface
  - Zero-overhead integration with HRLTrainer
  - Robust edge case handling (empty data, single values, zero variance)
  - Context manager support for automatic cleanup
  - Optional logging (can be disabled for tests)
- TensorBoard integration in HRLTrainer (`src/training/hrl_trainer.py`)
  - Optional logger parameter in __init__
  - Automatic episode-level action and goal tracking
  - Automatic logging after each episode (metrics, distributions, losses)
  - Enhanced progress monitoring with logged metrics
- TensorBoard integration in train.py
  - Command-line options: --log-dir, --no-logging
  - Automatic experiment naming with configuration and seed
  - Comprehensive hyperparameter logging (environment, training, reward configs)
  - Instructions for viewing logs with TensorBoard
- Logging usage example (`examples/logging_usage.py`)
  - Complete demonstration of TensorBoard logging
  - Shows hyperparameter logging, training with logging, and viewing results
  - Updated examples/README.md with logging example documentation
- Updated requirements.txt with tensorboard>=2.14.0
- Updated README.md with comprehensive logging documentation
  - ExperimentLogger usage examples
  - TensorBoard integration guide
  - What gets logged and how to view it
- BudgetEnv edge case tests (`tests/test_budget_env.py`)
  - 19 comprehensive edge case tests covering extreme financial scenarios
  - Very low income scenarios (barely covers expenses)
  - Extremely low income (immediate failure when expenses exceed income)
  - Very high fixed expenses (90% of income)
  - Very high variable expenses with high variance (std = 80% of mean)
  - Extreme positive inflation (50% monthly hyperinflation)
  - Extreme negative inflation (20% monthly deflation)
  - Zero inflation (constant expenses)
  - Maximum episode length (120 months / 10 years)
  - Very long episodes with compounding inflation effects
  - Single-step episodes (max_months=1)
  - High initial cash buffer scenarios ($50,000 starting cash)
  - Zero initial cash survival tests
  - Extreme variable expense variance testing
  - Combined extreme conditions (multiple stressors simultaneously)
  - Tests verify system robustness under stress conditions
  - Tests ensure no crashes or undefined behavior in edge cases
  - Tests validate proper handling of boundary values
- Sanity check tests for system-level validation (`tests/test_sanity_checks.py`)
  - 7 comprehensive test cases validating complete HRL system behavior
  - Random policy baseline validation (test_random_policy_does_not_accumulate_wealth)
  - Behavioral profile comparison tests:
    - Conservative vs aggressive cash balance comparison
    - Aggressive vs conservative investment comparison
    - Balanced profile positioning between extremes
  - Learning effectiveness validation (test_trained_policy_outperforms_random_policy)
  - Profile configuration validation:
    - Risk tolerance ordering across profiles
    - Reward coefficient ordering across profiles
  - Tests verify Requirements 1.1-1.3, 2.1-2.3, 6.2-6.3
  - Uses realistic training durations (20-30 episodes) for faster execution
  - Statistical validation across multiple evaluation episodes
  - Confirms behavioral profile differentiation and learning effectiveness
- Comprehensive integration tests for HRLTrainer (`tests/test_hrl_trainer.py`)
  - 13 new integration tests covering complete HRL training pipeline
  - Test complete episode execution with all components working together
  - Test high-level/low-level coordination and goal updates at correct intervals
  - Test policy updates occur correctly (both low-level and high-level agents)
  - Test analytics integration throughout episode execution
  - Test episode buffer accumulation and state history tracking
  - Test reward engine integration during training
  - Test full training pipeline from start to finish (5 episodes)
  - Test evaluation after training integration
  - Test hierarchical coordination complete flow
  - Test batch size coordination for low-level updates
  - Test high_period coordination for high-level updates
  - Test policy improvement verification over time
  - Test all components (env, agents, reward engine, analytics) working together
- Test coverage documentation (`tests/TEST_COVERAGE.md`)
  - Comprehensive overview of all test suites (150+ test cases)
  - Detailed coverage breakdown by component
  - Test execution instructions
  - Edge case documentation
  - Test quality metrics and CI/CD guidelines
- Main training script (`train.py`) - Complete CLI tool for training the HRL system
  - Comprehensive command-line interface with argparse
  - Support for both YAML configuration files and behavioral profiles
  - Mutually exclusive --config and --profile options
  - Command-line options:
    - `--config PATH`: Load configuration from YAML file
    - `--profile {conservative,balanced,aggressive}`: Use predefined behavioral profile
    - `--episodes N`: Override number of training episodes
    - `--output DIR`: Specify output directory for models (default: models/)
    - `--eval-episodes N`: Number of evaluation episodes after training (default: 10)
    - `--save-interval N`: Save checkpoint every N episodes (default: 1000)
    - `--seed N`: Random seed for reproducibility
  - Configuration loading with error handling and validation
  - Configuration summary display before training
  - System initialization with component-by-component progress feedback
  - Training execution with progress monitoring every 100 episodes
  - Training summary with statistics over last 100 episodes (all 9 metrics)
  - Automatic model saving:
    - `{config_name}_high_agent.pt` - High-level agent (Strategist)
    - `{config_name}_low_agent.pt` - Low-level agent (Executor)
    - `{config_name}_history.json` - Complete training history
  - JSON serialization with numpy array conversion for training history
  - Optional evaluation after training with comprehensive metrics display
  - Helpful usage examples in --help output
  - Proper error handling for configuration errors
- Comprehensive unit tests for ConfigurationManager (`tests/test_config_manager.py`)
  - 50+ test cases covering all functionality and validation rules
  - Configuration loading tests (5 tests)
    - Valid configuration loading with all parameters
    - Partial configuration with default values
    - Missing file error handling
    - Empty file error handling
    - Malformed YAML error handling
  - Behavioral profile tests (5 tests)
    - Conservative profile parameter verification
    - Balanced profile parameter verification
    - Aggressive profile parameter verification
    - Case-insensitive profile name handling
    - Invalid profile name error handling
  - Environment validation tests (17 tests)
    - Income validation (negative, zero)
    - Expense validation (fixed, variable mean, variable std)
    - Inflation validation (below range, above range, boundary values)
    - Safety threshold validation (negative)
    - Max months validation (zero)
    - Initial cash validation (negative)
    - Risk tolerance validation (below range, above range, boundary values)
  - Training validation tests (13 tests)
    - Num episodes validation (zero)
    - Gamma_low validation (below range, above range, boundary values)
    - Gamma_high validation (above range, boundary values)
    - High period validation (zero)
    - Batch size validation (zero)
    - Learning rate validation (zero, negative)
  - Reward validation tests (8 tests)
    - All reward coefficients (alpha, beta, gamma, delta, lambda_, mu)
    - Negative value validation for all coefficients
    - Zero value acceptance for all coefficients
  - Configuration override tests (1 test)
    - Profile loading with custom parameter overrides
- Analytics Module implementation (`src/utils/analytics.py`)
  - AnalyticsModule class for tracking and computing performance metrics
  - Step-by-step data recording with `record_step()` method
    - Tracks states, actions, rewards, goals, and invested amounts
    - Automatic cash balance extraction from state[3]
    - Safe copying of numpy arrays to prevent reference issues
    - Optional goal and invested_amount parameters
  - Comprehensive metric computation with `compute_episode_metrics()`
    - Cumulative wealth growth: Sum of all invested amounts
    - Cash stability index: Percentage of months with positive balance (0-1)
    - Sharpe-like ratio: Mean balance / std balance for risk-adjusted performance
    - Goal adherence: Mean absolute difference between target_invest_ratio (goal[0]) and actual invest action (action[0])
    - Policy stability: Mean variance of actions over time
    - Robust edge case handling (empty data, single data point)
  - Reset functionality with `reset()` method for new episodes
  - Comprehensive docstrings and type hints throughout
  - Ready for integration with HRLTrainer evaluation method
- Comprehensive unit tests for AnalyticsModule (`tests/test_analytics.py`)
  - 18 test cases covering all functionality and edge cases
  - Initialization and basic recording tests
  - Metric computation tests for all 5 metrics
  - Edge case tests (empty data, single step, negative cash)
  - Goal adherence tests (with/without goals, mismatched lengths)
  - Array copying verification to prevent reference issues
  - Reset functionality tests
  - Zero variance handling tests (Sharpe ratio, policy stability)
  - Comprehensive coverage of all code paths
- HRLTrainer complete training loop implementation
  - Full `train(num_episodes)` method in `src/training/hrl_trainer.py`
  - Episode execution with environment reset and state initialization
  - Initial goal generation from high-level agent
  - Monthly action execution by low-level agent
  - Automatic low-level policy updates when buffer reaches batch size
  - High-level re-planning every `high_period` steps (default: 6)
  - High-level reward computation over strategic periods
  - High-level policy updates with transition management
  - Final policy updates at episode termination
  - Progress monitoring with status updates every 100 episodes
  - Complete training history return with all metrics
- HRLTrainer class structure implementation
  - Training orchestrator initialization in `src/training/hrl_trainer.py`
  - Episode buffer for storing low-level transitions
  - State history tracking for high-level agent aggregation
  - Training metrics tracking (episode_rewards, episode_lengths, cash_balances, total_invested, low_level_losses, high_level_losses)
  - Comprehensive docstrings explaining HRL training coordination
  - Accepts BudgetEnv, FinancialStrategist, BudgetExecutor, RewardEngine, and TrainingConfig
- FinancialStrategist (High-Level Agent) implementation
  - HIRO-style agent for strategic goal generation
  - Custom StrategistNetwork with [64, 64] hidden layers
  - 5-dimensional aggregated state input (avg_cash, avg_investment_return, spending_trend, current_wealth, months_elapsed)
  - 3-dimensional goal output with automatic constraint enforcement
  - aggregate_state() method for computing macro features from state history
  - select_goal() method for generating strategic goals with sigmoid/softplus constraints
  - learn() method implementing simplified HIRO algorithm
  - Discount factor γ_high = 0.99 for long-term strategic planning
  - Gradient clipping for training stability
  - Model save/load functionality for checkpointing
  - Training metrics tracking (loss, policy entropy)
- Unit tests for FinancialStrategist (`tests/test_financial_strategist.py`)
  - Initialization tests
  - State aggregation tests (basic, empty, single state, averages)
  - Goal generation tests (basic, valid ranges, deterministic, invalid dimensions)
  - Learning tests (basic, empty, single transition, terminal states)
  - Policy update mechanics tests
  - Different states influence tests
- BudgetExecutor (Low-Level Agent) implementation
  - PPO-based agent for monthly allocation decisions
  - Custom PolicyNetwork with [128, 128] hidden layers and softmax output
  - 10-dimensional input (7-dimensional state + 3-dimensional goal)
  - 3-dimensional continuous action output with automatic normalization
  - act() method for action generation with deterministic mode support
  - learn() method implementing simplified policy gradient with PPO
  - Discount factor γ_low = 0.95 for temporal credit assignment
  - Entropy bonus for exploration (0.01 coefficient)
  - Model save/load functionality for checkpointing
  - Input validation for state and goal dimensions
  - Training metrics tracking (loss, policy entropy)
- Unit tests for BudgetExecutor (`tests/test_budget_executor.py`)
  - Initialization tests
  - Action generation tests (basic, deterministic, input concatenation)
  - Action normalization tests (including negative values)
  - Learning tests (basic, empty, single transition, terminal states)
  - Policy update mechanics tests
  - Goal influence tests
- BudgetEnv Gymnasium environment implementation
  - 7-dimensional continuous state space
  - 3-dimensional continuous action space with automatic normalization
  - Variable expense sampling from normal distribution
  - Inflation adjustments applied each step
  - Episode termination on negative cash or max months
  - Comprehensive info dictionary with financial metrics
  - Integrated RewardEngine for multi-objective reward computation
- RewardEngine implementation
  - Multi-objective reward computation for low-level agent
  - Strategic reward aggregation for high-level agent
  - Configurable reward coefficients (α, β, γ, δ, λ, μ)
  - Investment rewards, stability penalties, overspend penalties, debt penalties
  - Wealth growth tracking and stability bonus computation
- Environment module public API (`src/environment/__init__.py`)
- Basic usage example (`examples/basic_budget_env_usage.py`)
- Examples documentation (`examples/README.md`)
- RewardEngine usage example (`examples/reward_engine_usage.py`)
- Unit tests for BudgetEnv (`tests/test_budget_env.py`)
- Unit tests for RewardEngine (`tests/test_reward_engine.py`)

### Changed
- Marked Task 8 (Implement Analytics Module) as complete in tasks.md (except unit tests)
- Updated README.md with AnalyticsModule usage examples and comprehensive metrics documentation
- Updated design document with complete AnalyticsModule implementation details
- Updated HLD/LLD document with AnalyticsModule status and integration roadmap
- Marked Task 7.2 (Implement main training loop) as complete in tasks.md
- Marked Task 7.3 (Implement policy update coordination) as complete in tasks.md
- Updated implementation status in HLD/LLD document to reflect HRLTrainer training loop completion
- Updated README.md with complete HRLTrainer usage examples and training process documentation
- Marked Task 7.1 (Create HRLTrainer class structure) as complete in tasks.md
- Updated implementation status in HLD/LLD document to reflect HRLTrainer progress
- Marked Task 6 (Implement High-Level Agent) as complete in tasks.md
- Updated README.md with FinancialStrategist usage examples and API documentation
- Updated HLD/LLD document with FinancialStrategist implementation details
- Marked Task 5 (Implement Low-Level Agent) as complete in tasks.md
- Updated README.md with BudgetExecutor usage examples and API documentation
- Updated HLD/LLD document with BudgetExecutor implementation status
- Integrated RewardEngine with BudgetEnv for production-ready reward computation
- BudgetEnv now accepts optional RewardConfig parameter in constructor
- BudgetEnv.step() now uses RewardEngine.compute_low_level_reward() for all reward calculations
- Updated README.md with BudgetEnv and RewardEngine usage examples and API documentation
- Updated HLD/LLD document with implementation details and status tracking
- Updated project structure documentation to reflect new examples directory
- Marked Task 2 (Implement BudgetEnv) as complete in tasks.md
- Marked Task 3 (Implement Reward Engine) as complete in tasks.md
- Marked Task 4 (Integrate RewardEngine with BudgetEnv) as complete in tasks.md

### Documentation
- Added Quick Start section to README.md
- Added detailed BudgetEnv API documentation
- Added implementation status section to HLD/LLD document
- Created examples directory with usage demonstrations

## [0.1.0] - 2025-11-03

### Added
- Initial project structure
- Configuration system with dataclasses
  - EnvironmentConfig
  - TrainingConfig
  - RewardConfig
  - BehavioralProfile enum
- Core data models
  - Transition dataclass
- Project documentation
  - Requirements document
  - Design document
  - Implementation tasks
  - HLD/LLD document
- Package initialization files
- Dependencies in requirements.txt
  - gymnasium>=0.29.0
  - numpy>=1.24.0
  - stable-baselines3>=2.0.0
  - torch>=2.0.0
  - pyyaml>=6.0
  - tensorboard>=2.14.0

### Documentation
- Created comprehensive requirements document
- Created detailed design document with architecture diagrams
- Created implementation task breakdown
- Created README.md with project overview

---

## Version History

- **0.1.0** (2025-11-03): Initial project setup with configuration system and documentation
- **Unreleased**: BudgetEnv, RewardEngine, BudgetExecutor, FinancialStrategist, HRLTrainer (training loop + evaluation), AnalyticsModule, ConfigurationManager, and main training script (train.py) implementations complete with full integration and comprehensive tests. Test suite includes 176+ test cases: 34 for BudgetEnv (including 19 edge case tests), 18 for AnalyticsModule, 50+ for ConfigurationManager, 30+ for HRLTrainer (including 13 integration tests), and 7 sanity checks for system-level validation. Core HRL training system is fully functional with complete configuration management, validation, CLI training tool, behavioral profile validation, and comprehensive test coverage including extreme condition handling. The system can now be trained end-to-end using `python train.py --profile balanced`. Sanity checks verify learning effectiveness and behavioral profile differentiation. BudgetEnv edge case tests ensure robustness under extreme financial scenarios (hyperinflation, deflation, income shortfalls, combined stress conditions). Remaining: evaluation script (evaluate.py) for loading and testing trained models.
