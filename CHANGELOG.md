# Changelog

All notable changes to the Personal Finance Optimization HRL System will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Main training script (`train.py`) - Complete CLI tool for training the HRL system
  - Comprehensive command-line interface with argparse
  - Support for both YAML configuration files and behavioral profiles
  - Mutually exclusive --config and --profile options
  - Command-line options:
    - `--config PATH`: Load configuration from YAML file
    - `--profile {conservative,balanced,aggressive}`: Use predefined behavioral profile
    - `--episodes N`: Override number of training episodes
    - `--output DIR`: Specify output directory for models (default: models/)
    - `--eval-episodes N`: Number of evaluation episodes after training (default: 10)
    - `--save-interval N`: Save checkpoint every N episodes (default: 1000)
    - `--seed N`: Random seed for reproducibility
  - Configuration loading with error handling and validation
  - Configuration summary display before training
  - System initialization with component-by-component progress feedback
  - Training execution with progress monitoring every 100 episodes
  - Training summary with statistics over last 100 episodes (all 9 metrics)
  - Automatic model saving:
    - `{config_name}_high_agent.pt` - High-level agent (Strategist)
    - `{config_name}_low_agent.pt` - Low-level agent (Executor)
    - `{config_name}_history.json` - Complete training history
  - JSON serialization with numpy array conversion for training history
  - Optional evaluation after training with comprehensive metrics display
  - Helpful usage examples in --help output
  - Proper error handling for configuration errors
- Comprehensive unit tests for ConfigurationManager (`tests/test_config_manager.py`)
  - 50+ test cases covering all functionality and validation rules
  - Configuration loading tests (5 tests)
    - Valid configuration loading with all parameters
    - Partial configuration with default values
    - Missing file error handling
    - Empty file error handling
    - Malformed YAML error handling
  - Behavioral profile tests (5 tests)
    - Conservative profile parameter verification
    - Balanced profile parameter verification
    - Aggressive profile parameter verification
    - Case-insensitive profile name handling
    - Invalid profile name error handling
  - Environment validation tests (17 tests)
    - Income validation (negative, zero)
    - Expense validation (fixed, variable mean, variable std)
    - Inflation validation (below range, above range, boundary values)
    - Safety threshold validation (negative)
    - Max months validation (zero)
    - Initial cash validation (negative)
    - Risk tolerance validation (below range, above range, boundary values)
  - Training validation tests (13 tests)
    - Num episodes validation (zero)
    - Gamma_low validation (below range, above range, boundary values)
    - Gamma_high validation (above range, boundary values)
    - High period validation (zero)
    - Batch size validation (zero)
    - Learning rate validation (zero, negative)
  - Reward validation tests (8 tests)
    - All reward coefficients (alpha, beta, gamma, delta, lambda_, mu)
    - Negative value validation for all coefficients
    - Zero value acceptance for all coefficients
  - Configuration override tests (1 test)
    - Profile loading with custom parameter overrides
- Analytics Module implementation (`src/utils/analytics.py`)
  - AnalyticsModule class for tracking and computing performance metrics
  - Step-by-step data recording with `record_step()` method
    - Tracks states, actions, rewards, goals, and invested amounts
    - Automatic cash balance extraction from state[3]
    - Safe copying of numpy arrays to prevent reference issues
    - Optional goal and invested_amount parameters
  - Comprehensive metric computation with `compute_episode_metrics()`
    - Cumulative wealth growth: Sum of all invested amounts
    - Cash stability index: Percentage of months with positive balance (0-1)
    - Sharpe-like ratio: Mean balance / std balance for risk-adjusted performance
    - Goal adherence: Mean absolute difference between target_invest_ratio (goal[0]) and actual invest action (action[0])
    - Policy stability: Mean variance of actions over time
    - Robust edge case handling (empty data, single data point)
  - Reset functionality with `reset()` method for new episodes
  - Comprehensive docstrings and type hints throughout
  - Ready for integration with HRLTrainer evaluation method
- Comprehensive unit tests for AnalyticsModule (`tests/test_analytics.py`)
  - 18 test cases covering all functionality and edge cases
  - Initialization and basic recording tests
  - Metric computation tests for all 5 metrics
  - Edge case tests (empty data, single step, negative cash)
  - Goal adherence tests (with/without goals, mismatched lengths)
  - Array copying verification to prevent reference issues
  - Reset functionality tests
  - Zero variance handling tests (Sharpe ratio, policy stability)
  - Comprehensive coverage of all code paths
- HRLTrainer complete training loop implementation
  - Full `train(num_episodes)` method in `src/training/hrl_trainer.py`
  - Episode execution with environment reset and state initialization
  - Initial goal generation from high-level agent
  - Monthly action execution by low-level agent
  - Automatic low-level policy updates when buffer reaches batch size
  - High-level re-planning every `high_period` steps (default: 6)
  - High-level reward computation over strategic periods
  - High-level policy updates with transition management
  - Final policy updates at episode termination
  - Progress monitoring with status updates every 100 episodes
  - Complete training history return with all metrics
- HRLTrainer class structure implementation
  - Training orchestrator initialization in `src/training/hrl_trainer.py`
  - Episode buffer for storing low-level transitions
  - State history tracking for high-level agent aggregation
  - Training metrics tracking (episode_rewards, episode_lengths, cash_balances, total_invested, low_level_losses, high_level_losses)
  - Comprehensive docstrings explaining HRL training coordination
  - Accepts BudgetEnv, FinancialStrategist, BudgetExecutor, RewardEngine, and TrainingConfig
- FinancialStrategist (High-Level Agent) implementation
  - HIRO-style agent for strategic goal generation
  - Custom StrategistNetwork with [64, 64] hidden layers
  - 5-dimensional aggregated state input (avg_cash, avg_investment_return, spending_trend, current_wealth, months_elapsed)
  - 3-dimensional goal output with automatic constraint enforcement
  - aggregate_state() method for computing macro features from state history
  - select_goal() method for generating strategic goals with sigmoid/softplus constraints
  - learn() method implementing simplified HIRO algorithm
  - Discount factor γ_high = 0.99 for long-term strategic planning
  - Gradient clipping for training stability
  - Model save/load functionality for checkpointing
  - Training metrics tracking (loss, policy entropy)
- Unit tests for FinancialStrategist (`tests/test_financial_strategist.py`)
  - Initialization tests
  - State aggregation tests (basic, empty, single state, averages)
  - Goal generation tests (basic, valid ranges, deterministic, invalid dimensions)
  - Learning tests (basic, empty, single transition, terminal states)
  - Policy update mechanics tests
  - Different states influence tests
- BudgetExecutor (Low-Level Agent) implementation
  - PPO-based agent for monthly allocation decisions
  - Custom PolicyNetwork with [128, 128] hidden layers and softmax output
  - 10-dimensional input (7-dimensional state + 3-dimensional goal)
  - 3-dimensional continuous action output with automatic normalization
  - act() method for action generation with deterministic mode support
  - learn() method implementing simplified policy gradient with PPO
  - Discount factor γ_low = 0.95 for temporal credit assignment
  - Entropy bonus for exploration (0.01 coefficient)
  - Model save/load functionality for checkpointing
  - Input validation for state and goal dimensions
  - Training metrics tracking (loss, policy entropy)
- Unit tests for BudgetExecutor (`tests/test_budget_executor.py`)
  - Initialization tests
  - Action generation tests (basic, deterministic, input concatenation)
  - Action normalization tests (including negative values)
  - Learning tests (basic, empty, single transition, terminal states)
  - Policy update mechanics tests
  - Goal influence tests
- BudgetEnv Gymnasium environment implementation
  - 7-dimensional continuous state space
  - 3-dimensional continuous action space with automatic normalization
  - Variable expense sampling from normal distribution
  - Inflation adjustments applied each step
  - Episode termination on negative cash or max months
  - Comprehensive info dictionary with financial metrics
  - Integrated RewardEngine for multi-objective reward computation
- RewardEngine implementation
  - Multi-objective reward computation for low-level agent
  - Strategic reward aggregation for high-level agent
  - Configurable reward coefficients (α, β, γ, δ, λ, μ)
  - Investment rewards, stability penalties, overspend penalties, debt penalties
  - Wealth growth tracking and stability bonus computation
- Environment module public API (`src/environment/__init__.py`)
- Basic usage example (`examples/basic_budget_env_usage.py`)
- Examples documentation (`examples/README.md`)
- RewardEngine usage example (`examples/reward_engine_usage.py`)
- Unit tests for BudgetEnv (`tests/test_budget_env.py`)
- Unit tests for RewardEngine (`tests/test_reward_engine.py`)

### Changed
- Marked Task 8 (Implement Analytics Module) as complete in tasks.md (except unit tests)
- Updated README.md with AnalyticsModule usage examples and comprehensive metrics documentation
- Updated design document with complete AnalyticsModule implementation details
- Updated HLD/LLD document with AnalyticsModule status and integration roadmap
- Marked Task 7.2 (Implement main training loop) as complete in tasks.md
- Marked Task 7.3 (Implement policy update coordination) as complete in tasks.md
- Updated implementation status in HLD/LLD document to reflect HRLTrainer training loop completion
- Updated README.md with complete HRLTrainer usage examples and training process documentation
- Marked Task 7.1 (Create HRLTrainer class structure) as complete in tasks.md
- Updated implementation status in HLD/LLD document to reflect HRLTrainer progress
- Marked Task 6 (Implement High-Level Agent) as complete in tasks.md
- Updated README.md with FinancialStrategist usage examples and API documentation
- Updated HLD/LLD document with FinancialStrategist implementation details
- Marked Task 5 (Implement Low-Level Agent) as complete in tasks.md
- Updated README.md with BudgetExecutor usage examples and API documentation
- Updated HLD/LLD document with BudgetExecutor implementation status
- Integrated RewardEngine with BudgetEnv for production-ready reward computation
- BudgetEnv now accepts optional RewardConfig parameter in constructor
- BudgetEnv.step() now uses RewardEngine.compute_low_level_reward() for all reward calculations
- Updated README.md with BudgetEnv and RewardEngine usage examples and API documentation
- Updated HLD/LLD document with implementation details and status tracking
- Updated project structure documentation to reflect new examples directory
- Marked Task 2 (Implement BudgetEnv) as complete in tasks.md
- Marked Task 3 (Implement Reward Engine) as complete in tasks.md
- Marked Task 4 (Integrate RewardEngine with BudgetEnv) as complete in tasks.md

### Documentation
- Added Quick Start section to README.md
- Added detailed BudgetEnv API documentation
- Added implementation status section to HLD/LLD document
- Created examples directory with usage demonstrations

## [0.1.0] - 2025-11-03

### Added
- Initial project structure
- Configuration system with dataclasses
  - EnvironmentConfig
  - TrainingConfig
  - RewardConfig
  - BehavioralProfile enum
- Core data models
  - Transition dataclass
- Project documentation
  - Requirements document
  - Design document
  - Implementation tasks
  - HLD/LLD document
- Package initialization files
- Dependencies in requirements.txt
  - gymnasium>=0.29.0
  - numpy>=1.24.0
  - stable-baselines3>=2.0.0
  - torch>=2.0.0
  - pyyaml>=6.0

### Documentation
- Created comprehensive requirements document
- Created detailed design document with architecture diagrams
- Created implementation task breakdown
- Created README.md with project overview

---

## Version History

- **0.1.0** (2025-11-03): Initial project setup with configuration system and documentation
- **Unreleased**: BudgetEnv, RewardEngine, BudgetExecutor, FinancialStrategist, HRLTrainer (training loop + evaluation), AnalyticsModule, ConfigurationManager, and main training script (train.py) implementations complete with full integration and comprehensive tests (18 test cases for AnalyticsModule, 50+ test cases for ConfigurationManager). Core HRL training system is fully functional with complete configuration management, validation, and CLI training tool. The system can now be trained end-to-end using `python train.py --profile balanced`. Remaining: integration tests and evaluation script (evaluate.py).
